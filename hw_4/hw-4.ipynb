{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b5a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a353286",
   "metadata": {},
   "source": [
    "In this homework, we will use the Car price dataset like last week. Download it from [here](https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv).\n",
    "\n",
    "Or you can do it with `wget`:\n",
    "\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv\n",
    "```\n",
    "\n",
    "We'll work with the `MSRP` variable, and we'll transform it to a classification task. \n",
    "\n",
    "For the rest of the homework, you'll need to use only these columns:\n",
    "\n",
    "* `Make`,\n",
    "* `Model`,\n",
    "* `Year`,\n",
    "* `Engine HP`,\n",
    "* `Engine Cylinders`,\n",
    "* `Transmission Type`,\n",
    "* `Vehicle Style`,\n",
    "* `highway MPG`,\n",
    "* `city mpg`\n",
    "* `MSRP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ed5ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-06 13:12:35--  https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1475504 (1,4M) [text/plain]\n",
      "Saving to: ‘data.csv.1’\n",
      "\n",
      "data.csv.1          100%[===================>]   1,41M  3,30MB/s    in 0,4s    \n",
      "\n",
      "2023-10-06 13:12:36 (3,30 MB/s) - ‘data.csv.1’ saved [1475504/1475504]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv'\n",
    "!wget $data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd32348",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "* Keep only the columns above\n",
    "* Lowercase the column names and replace spaces with underscores\n",
    "* Fill the missing values with 0 \n",
    "* Make the price binary (1 if above the average, 0 otherwise) - this will be our target variable `above_average`\n",
    "\n",
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` function for that with `random_state=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4629daea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7148, 2383, 2383)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Keep only the columns above\n",
    "columns = [\n",
    "    'Make',\n",
    "    'Model',\n",
    "    'Year',\n",
    "    'Engine HP',\n",
    "    'Engine Cylinders',\n",
    "    'Transmission Type',\n",
    "    'Vehicle Style',\n",
    "    'highway MPG',\n",
    "    'city mpg',\n",
    "    'MSRP',\n",
    "]\n",
    "df = df[columns]\n",
    "\n",
    "# Lowercase the column names and replace spaces with underscores\n",
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "\n",
    "# Fill the missing values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Make the price binary (1 if above the average, 0 otherwise) - this will be our target variable above_average\n",
    "price_mean = df['msrp'].mean()\n",
    "df['above_average'] = (df['msrp'] > price_mean).astype(int)\n",
    "del df['msrp']\n",
    "\n",
    "# Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution.\n",
    "# Use train_test_split function for that with random_state=1\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)\n",
    "\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac636b",
   "metadata": {},
   "source": [
    "### Question 1: ROC AUC feature importance\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "\n",
    "Let's do that\n",
    "\n",
    "* For each numerical variable, use it as score and compute AUC with the `above_average` variable\n",
    "* Use the training dataset for that\n",
    "\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. `-df_train['engine_hp']`)\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "- `engine_hp` +\n",
    "- `engine_cylinders`\n",
    "- `highway_mpg`\n",
    "- `city_mpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eaf151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engine_hp', 0.9157738865600598),\n",
       " ('engine_cylinders', 0.765740332665676),\n",
       " ('highway_mpg', 0.6307493974364577),\n",
       " ('city_mpg', 0.6706787971410986)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = [\n",
    "    'engine_hp',\n",
    "    'engine_cylinders',\n",
    "    'highway_mpg',\n",
    "    'city_mpg',\n",
    "]\n",
    "\n",
    "f_aucs = []\n",
    "\n",
    "for f in selected_features:\n",
    "    auc_score = roc_auc_score(df['above_average'], df[f])\n",
    "    \n",
    "    # If AUC is less than 0.5, invert the variable and recompute the AUC\n",
    "    if auc_score < 0.5:\n",
    "        auc_score = roc_auc_score(df['above_average'], -df[f])\n",
    "        \n",
    "    f_aucs.append((f, auc_score))\n",
    "    \n",
    "f_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5847b4",
   "metadata": {},
   "source": [
    "### Question 2: Training the model\n",
    "\n",
    "Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "```python\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "- 0.678\n",
    "- 0.779\n",
    "- 0.878\n",
    "- 0.979 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70177015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical = ['year', 'engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg']\n",
    "categorical = ['make', 'model', 'transmission_type', 'vehicle_style']\n",
    "\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dict)\n",
    "\n",
    "X_train = dv.transform(train_dict)\n",
    "y_train = df_train['above_average']\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "y_val = df_val['above_average']\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "round(auc_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9fb17",
   "metadata": {},
   "source": [
    "### Question 3: Precision and Recall\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "* For each threshold, compute precision and recall\n",
    "* Plot them\n",
    "\n",
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "* 0.28\n",
    "* 0.48\n",
    "* 0.68\n",
    "* 0.88 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6e300f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     precisions\u001b[38;5;241m.\u001b[39mappend(P)\n\u001b[1;32m     22\u001b[0m     recalls\u001b[38;5;241m.\u001b[39mappend(R)\n\u001b[0;32m---> 23\u001b[0m     metrics\u001b[38;5;241m.\u001b[39mappend((t, P, R))\n\u001b[1;32m     25\u001b[0m intersection_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((t \u001b[38;5;28;01mfor\u001b[39;00m t, p, r \u001b[38;5;129;01min\u001b[39;00m metrics \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(p, r, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(thresholds, precisions, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 101)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in thresholds:\n",
    "    predict_above_avarage = (y_pred >= t)\n",
    "    actual_above_avarage = (y_val == 1)\n",
    "    actual_below_avarage = (y_val == 0)\n",
    "    \n",
    "    true_positive = (predict_above_avarage & actual_above_avarage).sum()\n",
    "    false_positive = (predict_above_avarage & actual_below_avarage).sum()\n",
    "    false_negative = (~predict_above_avarage & actual_above_avarage).sum()\n",
    "    \n",
    "    if true_positive + false_positive == 0:\n",
    "        P = np.nan\n",
    "    else:\n",
    "        P = true_positive / (true_positive + false_positive)\n",
    "    \n",
    "    R = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    precisions.append(P)\n",
    "    recalls.append(R)\n",
    "    metrics.append((t, P, R))\n",
    "\n",
    "intersection_threshold = next((t for t, p, r in metrics if np.isclose(p, r, atol=0.01)), None)\n",
    "\n",
    "plt.plot(thresholds, precisions, label='Precision', color='blue')\n",
    "plt.plot(thresholds, recalls, label='Recall', color='red')\n",
    "\n",
    "if intersection_threshold:\n",
    "    idx = thresholds.tolist().index(intersection_threshold)\n",
    "    plt.scatter(intersection_threshold, precisions[idx], color='green', marker='o', s=100, zorder=5)\n",
    "    print(f\"Intersection point: Threshold={intersection_threshold:.2f}, Precision={precisions[idx]:.2f}, Recall={recalls[idx]:.2f}\")\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8fa08",
   "metadata": {},
   "source": [
    "### Question 4: F1 score\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "At which threshold F1 is maximal?\n",
    "\n",
    "- 0.12\n",
    "- 0.32\n",
    "- 0.52 +\n",
    "- 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7310237",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m q4 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.12\u001b[39m, \u001b[38;5;241m0.32\u001b[39m, \u001b[38;5;241m0.52\u001b[39m, \u001b[38;5;241m0.72\u001b[39m]\n\u001b[1;32m      2\u001b[0m q4_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, P, R \u001b[38;5;129;01min\u001b[39;00m q4:\n\u001b[1;32m      4\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m P \u001b[38;5;241m*\u001b[39m R \u001b[38;5;241m/\u001b[39m (P \u001b[38;5;241m+\u001b[39m R)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m q4:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "q4 = [0.12, 0.32, 0.52, 0.72]\n",
    "q4_res = []\n",
    "for t, P, R in metrics:\n",
    "    f1 = 2 * P * R / (P + R)\n",
    "    if t in q4:\n",
    "        q4_res.append((t, f1))\n",
    "\n",
    "max(q4_res, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389644e9",
   "metadata": {},
   "source": [
    "### Question 5: 5-Fold CV\n",
    "\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "* Iterate over different folds of `df_full_train`\n",
    "* Split the data into train and validation\n",
    "* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "* Use AUC to evaluate the model on validation\n",
    "\n",
    "How large is standard devidation of the scores across different folds?\n",
    "\n",
    "- 0.003 +\n",
    "- 0.030\n",
    "- 0.090\n",
    "- 0.140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd853a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, y, C=1.0):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    dv.fit(cat)\n",
    "    \n",
    "    X = dv.transform(cat)\n",
    "    \n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    return dv, model\n",
    "\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    cat = df[categorical + numerical].to_dict(orient='records')\n",
    "    X = dv.transform(cat)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "aucs = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_train_full):\n",
    "    df_train = df_train_full.iloc[train_idx]\n",
    "    df_val = df_train_full.iloc[val_idx]\n",
    "    \n",
    "    y_train = df_train.above_average.values\n",
    "    y_val = df_val.above_average.values\n",
    "    \n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    aucs.append(auc)\n",
    "\n",
    "print('auc = %0.3f ± %0.3f' % (np.mean(aucs), np.std(aucs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf108aa",
   "metadata": {},
   "source": [
    "### Question 6: Hyperparemeter Tuning\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter `C`\n",
    "\n",
    "* Iterate over the following `C` values: `[0.01, 0.1, 0.5, 10]`\n",
    "* Initialize `KFold` with the same parameters as previously\n",
    "* Use these parametes for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "* Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "\n",
    "Which `C` leads to the best mean score?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 0.5\n",
    "- 10 +\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.01, 0.1, 0.5, 10]\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "means = []\n",
    "\n",
    "for C in Cs:\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_train_full):\n",
    "        df_train = df_train_full.iloc[train_idx]\n",
    "        df_val = df_train_full.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.above_average.values\n",
    "        y_val = df_val.above_average.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    m = np.mean(aucs)\n",
    "    \n",
    "    means.append((C, m))\n",
    "\n",
    "max(means, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff1533",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "- Submit your results here: https://forms.gle/E7Fa3WuBw3HkPQYg6\n",
    "- If your answer doesn't match options exactly, select the closest one.\n",
    "- You can submit your solution multiple times. In this case, only the last submission will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073925c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
